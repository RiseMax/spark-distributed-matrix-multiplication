from pyspark.sql import SparkSession
from pyspark import Broadcast
import numpy as np
import time
import random
from typing import Tuple, List
import os
import sys

# å®šä¹‰MatrixEntryç±»ï¼ˆé€‚é…æµ®ç‚¹å‹ç´¢å¼•å’Œå€¼ï¼‰
class MatrixEntry:
    def __init__(self, row: float, col: float, value: float):
        self.row = row
        self.col = col
        self.value = value
    
    def __repr__(self):
        return f"MatrixEntry({self.row:.2f}, {self.col:.2f}, {self.value:.6f})"

# ===================== è¾…åŠ©å‡½æ•°ï¼šå¯è§†åŒ–ä¸è¿›åº¦æç¤º =====================
def print_progress(msg: str, symbol: str = "="):
    """æ‰“å°å¸¦æ ¼å¼çš„è¿›åº¦æç¤ºï¼Œå¢å¼ºå¯è§†åŒ–æ•ˆæœ"""
    print(f"\n{symbol * 20} {msg} {symbol * 20}")

def preview_matrix_rdd(rdd, matrix_name: str, row_num: int = 5, col_num: int = 5):
    """é¢„è§ˆçŸ©é˜µRDDçš„å‰Nè¡ŒNåˆ—æ•°æ®ï¼Œå¯è§†åŒ–å±•ç¤º"""
    try:
        # æ”¶é›†çŸ©é˜µæ•°æ®å¹¶è½¬æ¢ä¸ºäºŒç»´æ•°ç»„
        matrix_data = rdd.collect()
        if not matrix_data:
            print(f"{matrix_name} çŸ©é˜µæ— æœ‰æ•ˆæ•°æ®å¯é¢„è§ˆ")
            return
        
        # æå–è¡Œåˆ—ç´¢å¼•å’Œå€¼ï¼Œæ„å»ºäºŒç»´çŸ©é˜µ
        row_indices = sorted(list(set([int(x[0]) for x in matrix_data])))
        col_indices = sorted(list(set([int(x[1]) for x in matrix_data])))
        
        # åªé¢„è§ˆå‰row_numè¡Œå’Œcol_numåˆ—
        preview_rows = row_indices[:row_num]
        preview_cols = col_indices[:col_num]
        
        # æ„å»ºé¢„è§ˆçŸ©é˜µ
        preview_mat = np.zeros((len(preview_rows), len(preview_cols)))
        for (r, c, v) in matrix_data:
            r_int = int(r)
            c_int = int(c)
            if r_int in preview_rows and c_int in preview_cols:
                r_idx = preview_rows.index(r_int)
                c_idx = preview_cols.index(c_int)
                preview_mat[r_idx][c_idx] = v
        
        # æ‰“å°é¢„è§ˆä¿¡æ¯
        print(f"\n{matrix_name} çŸ©é˜µå‰ {len(preview_rows)} è¡Œ {len(preview_cols)} åˆ— é¢„è§ˆï¼š")
        print("=" * 40)
        print(preview_mat.round(4))  # ä¿ç•™4ä½å°æ•°ï¼Œæ›´æ¸…æ™°
        print("=" * 40)
        
    except Exception as e:
        print(f"{matrix_name} çŸ©é˜µé¢„è§ˆå¤±è´¥: {e}")

def print_matrix_info(matrix_name: str, rows: int, cols: int, data_count: int):
    """æ‰“å°çŸ©é˜µè¯¦ç»†ä¿¡æ¯"""
    print(f"\nã€{matrix_name} çŸ©é˜µä¿¡æ¯ã€‘")
    print(f"è¡Œæ•°ï¼š{rows}")
    print(f"åˆ—æ•°ï¼š{cols}")
    print(f"éé›¶å…ƒç´ æ•°ï¼š{data_count}")
    print(f"çŸ©é˜µå½¢çŠ¶ï¼š{rows} Ã— {cols}")

# ===================== 1. ç¯å¢ƒé…ç½®ï¼ˆé€‚é…é›†ç¾¤ç¯å¢ƒï¼Œå…¼å®¹æµ®ç‚¹å‹æ•°æ®å¤„ç†ï¼‰ =====================

# Spark Masteråœ°å€ï¼ˆDocker Composeä¸­masterå®¹å™¨çš„åœ°å€ï¼‰
SPARK_MASTER_URL = "spark://master:7077"
# Dockerç¯å¢ƒä¸‹Driverä¸Executoré€šä¿¡çš„åœ°å€
os.environ["SPARK_DRIVER_HOST"] = "host.docker.internal"

# å¼ºåˆ¶è¦†ç›–ç¯å¢ƒå˜é‡ï¼ˆExecutorç”¨å®¹å™¨Pythonï¼ŒDriverç”¨æœ¬åœ°Pythonï¼‰
#os.environ["PYSPARK_PYTHON"] = DOCKER_PYTHON_EXE
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable 

# ===================== 2. åˆå§‹åŒ–Sparké›†ç¾¤ï¼ˆè‡ªåŠ¨ä½¿ç”¨æœ€å¤§èµ„æºï¼Œç§»é™¤ç¡¬ç¼–ç é™åˆ¶ï¼‰ =====================
def init_spark() -> SparkSession:
    # é›†ç¾¤èµ„æºé…ç½®ï¼šå¯ç”¨åŠ¨æ€èµ„æºåˆ†é…ï¼ŒåŒæ—¶å¯ç”¨å¤–éƒ¨ShuffleæœåŠ¡ï¼Œè§£å†³ä¾èµ–æŠ¥é”™
    spark = SparkSession.builder \
        .appName("MatrixMultBroadcastTest-Cluster") \
        .master(SPARK_MASTER_URL) \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.driver.memory", "6g")\
        .config("spark.executor.memory", "6g")\
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    # æ‰“å°é›†ç¾¤ä¿¡æ¯ï¼ŒéªŒè¯èµ„æºé…ç½®
    print_progress("Sparké›†ç¾¤åˆå§‹åŒ–å®Œæˆ")
    print(f"Master URL: {spark.sparkContext.master}")
    print(f"Application ID: {spark.sparkContext.applicationId}")

    return spark

# ===================== 3. è¯»å–HDFSçŸ©é˜µæ–‡ä»¶ï¼ˆä»…ä¿®æ”¹ç»´åº¦è®¡ç®—éƒ¨åˆ†ï¼Œå…¶ä½™ä¸å˜ï¼‰ =====================
def read_matrix_from_file_csv(
    sc, 
    file_path: str,
    is_b_matrix: bool = False,  # æ˜¯å¦ä¸ºBçŸ©é˜µï¼ˆç”¨äºbroadcastä¼˜åŒ–ï¼‰
    matrix_rows: int = 1,       # æ‰‹åŠ¨æŒ‡å®šçŸ©é˜µè¡Œæ•°ï¼ˆè‹¥ä¸ºå•è¡ŒçŸ©é˜µï¼Œé»˜è®¤1ï¼›å¯æ ¹æ®å®é™…ä¿®æ”¹ï¼‰
    matrix_cols: int = None     # æ‰‹åŠ¨æŒ‡å®šçŸ©é˜µåˆ—æ•°ï¼ˆé»˜è®¤Noneï¼Œè‡ªåŠ¨ä»æ•°æ®ä¸­è·å–ï¼‰
) -> tuple:
    """
    ä»HDFSè¯»å–å•è¡Œå¤šåˆ—æµ®ç‚¹å‹CSVæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºäºŒç»´çŸ©é˜µRDD
    é€‚é…æ ¼å¼ï¼šCSVæ–‡ä»¶ä»…1è¡Œï¼ŒåŒ…å«å¤šä¸ªæµ®ç‚¹å‹æ•°å€¼ï¼ˆé€—å·åˆ†éš”ï¼‰
    è¿”å›ï¼š(çŸ©é˜µRDD, çŸ©é˜µå­—å…¸ï¼ˆä»…BçŸ©é˜µæœ‰ï¼‰, çŸ©é˜µè¡Œæ•°ï¼ˆæ•´æ•°ï¼‰, çŸ©é˜µåˆ—æ•°ï¼ˆæ•´æ•°ï¼‰)
    """
    # åŠ¨æ€åˆ†åŒºæ•°ï¼šåŸºäºé›†ç¾¤é»˜è®¤å¹¶è¡Œåº¦ï¼Œè‡ªåŠ¨é€‚é…æ–‡ä»¶å¤§å°
    total_cores = sc.defaultParallelism
    partitions = total_cores  # å¯æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´ä¸º total_cores * 2
    
    # å®‰å…¨æµ®ç‚¹è½¬æ¢è¾…åŠ©å‡½æ•°
    def safe_float_convert(val):
        try:
            return float(str(val).strip())
        except (ValueError, TypeError, Exception):
            return None
    
    # è¯»å–å¹¶è§£æå•è¡Œå¤šåˆ—CSVæ–‡ä»¶
    try:
        print_progress(f"å¼€å§‹è¯»å–å¹¶è§£ææ–‡ä»¶: {os.path.basename(file_path)}", "-")
        # ç¬¬ä¸€æ­¥ï¼šè¯»å–åŸå§‹æ–‡ä»¶ï¼Œè·å–å•è¡Œæ•°æ®
        raw_rdd = sc.textFile(file_path, partitions)
        print(f"åŸå§‹æ–‡ä»¶åˆ†åŒºæ•°ï¼š{raw_rdd.getNumPartitions()}")
        
        # è·å–å”¯ä¸€è¡Œæ•°æ®ï¼ˆè¿‡æ»¤ç©ºè¡Œåä»…ä¿ç•™ä¸€è¡Œï¼‰
        non_empty_rdd = raw_rdd.filter(lambda line: line.strip())
        line_count = non_empty_rdd.count()
        print(f"è¿‡æ»¤ç©ºè¡Œåå‰©ä½™è¡Œæ•°ï¼š{line_count}")
        
        if line_count != 1:
            print(f"è­¦å‘Šï¼šCSVæ–‡ä»¶ {file_path} ä¸æ˜¯å•è¡Œæ•°æ®ï¼Œå½“å‰æœ‰ {line_count} è¡Œ")
            return sc.emptyRDD(), {}, 0, 0
        
        # ç¬¬äºŒæ­¥ï¼šè§£æå•è¡Œæ•°æ®ä¸ºæµ®ç‚¹æ•°ç»„
        line_data = non_empty_rdd.first()  # è·å–å”¯ä¸€è¡Œ
        print(f"åŸå§‹è¡Œæ•°æ®é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼š{len(line_data)}")
        print(f"åŸå§‹è¡Œæ•°æ®å‰100ä¸ªå­—ç¬¦ï¼š{line_data[:100]}...")
        
        value_list = [safe_float_convert(val) for val in line_data.strip().split(",")]
        # è¿‡æ»¤è½¬æ¢å¤±è´¥çš„æ— æ•ˆå€¼
        valid_value_list = [v for v in value_list if v is not None]
        print(f"è§£æå‡ºæµ®ç‚¹æ•°å€¼æ€»æ•°ï¼š{len(value_list)}")
        print(f"æœ‰æ•ˆæµ®ç‚¹æ•°å€¼æ•°ï¼š{len(valid_value_list)}")
        
        # ç¬¬ä¸‰æ­¥ï¼šç¡®å®šçŸ©é˜µè¡Œåˆ—æ•°
        if matrix_cols is None:
            matrix_cols = len(valid_value_list)  # è‡ªåŠ¨è·å–åˆ—æ•°ï¼ˆå•è¡Œæ—¶ï¼Œåˆ—æ•°=æ•°å€¼ä¸ªæ•°ï¼‰
        print(f"æŒ‡å®šçŸ©é˜µè¡Œæ•°ï¼š{matrix_rows}ï¼Œè‡ªåŠ¨è®¡ç®—åˆ—æ•°ï¼š{matrix_cols}")
        
        # è‹¥æŒ‡å®šäº†å¤šè¡Œï¼Œéœ€ç¡®ä¿æ•°å€¼æ€»æ•°èƒ½è¢«è¡Œæ•°æ•´é™¤ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰
        if len(valid_value_list) % matrix_rows != 0:
            print(f"è­¦å‘Šï¼šæ•°å€¼æ€»æ•° {len(valid_value_list)} æ— æ³•è¢«æŒ‡å®šè¡Œæ•° {matrix_rows} æ•´é™¤ï¼Œå°†æˆªæ–­æ•°æ®")
            valid_value_list = valid_value_list[:matrix_rows * matrix_cols]
            print(f"æˆªæ–­åæ•°å€¼æ€»æ•°ï¼š{len(valid_value_list)}")
        
        # ç¬¬å››æ­¥ï¼šè½¬æ¢ä¸ºï¼ˆè¡Œç´¢å¼•, åˆ—ç´¢å¼•, å¯¹åº”å€¼ï¼‰çš„RDDæ ¼å¼ï¼ˆå…¼å®¹åç»­é€»è¾‘ï¼‰
        matrix_data = []
        for row_idx in range(matrix_rows):
            for col_idx in range(matrix_cols):
                # è®¡ç®—æ•°å€¼åœ¨ä¸€ç»´åˆ—è¡¨ä¸­çš„ç´¢å¼•
                val_idx = row_idx * matrix_cols + col_idx
                if val_idx < len(valid_value_list):
                    matrix_data.append((float(row_idx), float(col_idx), valid_value_list[val_idx]))
        
        # è½¬æ¢ä¸ºRDD
        matrix_rdd = sc.parallelize(matrix_data, partitions)
        data_count = matrix_rdd.count()
        
        # éªŒè¯æœ‰æ•ˆæ•°æ®
        if matrix_rdd.take(1):
            print(f"âœ… æˆåŠŸè¯»å–HDFS CSVæ–‡ä»¶: {file_path}")
            print(f"æ–‡ä»¶åˆ†åŒºæ•°ï¼š{matrix_rdd.getNumPartitions()}")
            print(f"æœ‰æ•ˆçŸ©é˜µæ•°æ®ï¼š{matrix_rows} è¡Œ Ã— {matrix_cols} åˆ—")
            print(f"çŸ©é˜µå…ƒç´ æ€»æ•°ï¼š{data_count}")
            # é¢„è§ˆçŸ©é˜µ
            preview_matrix_rdd(matrix_rdd, os.path.basename(file_path).split(".")[0])
        else:
            print(f"âŒ è­¦å‘Šï¼šCSVæ–‡ä»¶ {file_path} æ— æœ‰æ•ˆæ•°æ®")
            return sc.emptyRDD(), {}, 0, 0
            
    except Exception as e:
        print(f"âŒ è¯»å–/è§£æCSVæ–‡ä»¶å¤±è´¥: {e}")
        return sc.emptyRDD(), {}, 0, 0
    
    # è‡ªåŠ¨è®¡ç®—æ•´æ•°çŸ©é˜µç»´åº¦ï¼ˆç›´æ¥ä½¿ç”¨æŒ‡å®š/è‡ªåŠ¨è·å–çš„è¡Œåˆ—æ•°ï¼Œæ— éœ€å†é€šè¿‡ç´¢å¼•è®¡ç®—ï¼‰
    try:
        final_rows = matrix_rows
        final_cols = matrix_cols
        print(f"âœ… è‡ªåŠ¨è¯†åˆ«çŸ©é˜µç»´åº¦: {final_rows} è¡Œï¼ˆæ•´æ•°ï¼‰ Ã— {final_cols} åˆ—ï¼ˆæ•´æ•°ï¼‰")
        # è‹¥éœ€è¦è·å–ç´¢å¼•èŒƒå›´ï¼ˆå…¼å®¹åŸæœ‰æ—¥å¿—ï¼‰
        row_indices = [int(row_idx) for row_idx in range(final_rows)]
        col_indices = [int(col_idx) for col_idx in range(final_cols)]
        if row_indices and col_indices:
            print(f"åŸå§‹æµ®ç‚¹ç´¢å¼•èŒƒå›´ï¼šè¡Œç´¢å¼• [{min(row_indices)}, {max(row_indices)}]ï¼Œåˆ—ç´¢å¼• [{min(col_indices)}, {max(col_indices)}]")
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨è·å–çŸ©é˜µç»´åº¦å¤±è´¥: {e}")
        final_rows, final_cols = 0, 0
    
    # BçŸ©é˜µï¼šæŒ‰åˆ—å­˜å‚¨ä¸ºæµ®ç‚¹å‹å­—å…¸ï¼ˆåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
    matrix_dict = {}
    if is_b_matrix and not matrix_rdd.isEmpty():
        try:
            print_progress(f"å¼€å§‹æ„å»ºBçŸ©é˜µåˆ—å­—å…¸", "-")
            matrix_dict = matrix_rdd.map(lambda x: (int(round(x[1])), ((x[0], x[1]), x[2]))).groupByKey().collectAsMap()
            matrix_dict = {k: dict(v) for k, v in matrix_dict.items()}
            print(f"âœ… æˆåŠŸç”ŸæˆBçŸ©é˜µBroadcastå­—å…¸ï¼ˆåŒ…å« {len(matrix_dict)} åˆ—æ•°æ®ï¼‰")
        except Exception as e:
            print(f"âŒ ç”ŸæˆBçŸ©é˜µBroadcastå­—å…¸å¤±è´¥: {e}")
            matrix_dict = {}
    
    return matrix_rdd, matrix_dict, final_rows, final_cols


def read_matrix_from_file_txt(
    sc, 
    file_path: str,
    is_b_matrix: bool = False  # æ˜¯å¦ä¸ºBçŸ©é˜µï¼ˆç”¨äºbroadcastä¼˜åŒ–ï¼‰
) -> tuple:
    """
    ä»HDFSè¯»å–æµ®ç‚¹å‹çŸ©é˜µæ–‡ä»¶ï¼ŒåŠ¨æ€åˆ†åŒºï¼Œè‡ªåŠ¨è®¡ç®—æ•´æ•°çŸ©é˜µç»´åº¦
    é€‚é…æ ¼å¼ï¼šCSVæ–‡ä»¶ï¼Œæ¯è¡Œæ ¼å¼ä¸º(row_idx, col_idx, value)ï¼Œå‡ä¸ºæµ®ç‚¹å‹
    è¿”å›ï¼š(çŸ©é˜µRDD, çŸ©é˜µå­—å…¸ï¼ˆä»…BçŸ©é˜µæœ‰ï¼‰, çŸ©é˜µè¡Œæ•°ï¼ˆæ•´æ•°ï¼‰, çŸ©é˜µåˆ—æ•°ï¼ˆæ•´æ•°ï¼‰)
    ä¼˜åŒ–ç‚¹ï¼šæå‡è¯»å–é€Ÿåº¦ï¼Œä¿ç•™å¯è§†åŒ–è¿›åº¦å±•ç¤º
    """
    # è¾…åŠ©å‡½æ•°ï¼šæ‰“å°å¯è§†åŒ–è¿›åº¦
    def print_progress(msg: str, symbol: str = "-"):
        print(f"\n{symbol * 15} {msg} {symbol * 15}")
    
    # ===================== ä¼˜åŒ–1ï¼šæå‡åˆ†åŒºç­–ç•¥ï¼Œé€‚é…æ–‡ä»¶å¤§å° =====================
    # åŠ¨æ€åˆ†åŒºæ•°ï¼šé›†ç¾¤æ ¸å¿ƒæ•°çš„2~3å€ï¼ˆå……åˆ†åˆ©ç”¨å¹¶è¡Œèµ„æºï¼Œé¿å…å°åˆ†åŒº/å¤§åˆ†åŒºé—®é¢˜ï¼‰
    total_cores = sc.defaultParallelism
    partitions = max(total_cores * 2, 8)  # ä¿åº•8ä¸ªåˆ†åŒºï¼Œé¿å…æ ¸å¿ƒæ•°è¿‡å°‘å¯¼è‡´åˆ†åŒºä¸è¶³
    print_progress(f"åˆå§‹åŒ–è¯»å–é…ç½®ï¼šåˆ†åŒºæ•°={partitions}", "=")

    # ===================== ä¼˜åŒ–2ï¼šè®¾ç½®HDFSè¯»å–ä¼˜åŒ–å‚æ•°ï¼Œæå‡ä¼ è¾“é€Ÿåº¦ =====================
    hadoop_conf = sc._jsc.hadoopConfiguration()
    # å¢å¤§è¯»å–ç¼“å†²åŒºï¼Œæå‡å¤§æ–‡ä»¶è¯»å–é€Ÿåº¦
    hadoop_conf.set("io.file.buffer.size", "131072")  # 128KBï¼ˆé»˜è®¤4KBï¼‰
    # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ— æ•ˆç­‰å¾…
    hadoop_conf.set("dfs.client.read.timeout", "300000")  # 5åˆ†é’Ÿè¶…æ—¶
    hadoop_conf.set("dfs.socket.timeout", "300000")
    # æ³¨é‡Š/åˆ é™¤çŸ­è·¯è¯»å–é…ç½®ï¼ˆæ ¸å¿ƒä¿®å¤ç‚¹ï¼‰
    # hadoop_conf.set("dfs.client.read.shortcircuit", "true")  # ç¦ç”¨è¯¥åŠŸèƒ½ï¼Œé¿å…é…ç½®ç¼ºå¤±æŠ¥é”™
    print("å·²é…ç½®HDFSè¯»å–ä¼˜åŒ–å‚æ•°ï¼šå¢å¤§ç¼“å†²åŒºã€è®¾ç½®è¶…æ—¶")

    # ===================== ä¼˜åŒ–3ï¼šè½»é‡å‰ç½®æ ¡éªŒï¼Œé¿å…æ— æ•ˆå…¨é‡è¯»å– =====================
    print_progress("å¼€å§‹å‰ç½®æ–‡ä»¶æœ‰æ•ˆæ€§æ ¡éªŒ")
    try:
        # é€šè¿‡HDFS APIå¿«é€Ÿåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨/æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆæ— éœ€åŠ è½½æ•°æ®ï¼‰
        path = sc._jvm.org.apache.hadoop.fs.Path(file_path)
        fs = path.getFileSystem(hadoop_conf)
        if not fs.exists(path):
            print(f"âŒ é”™è¯¯ï¼šHDFSæ–‡ä»¶ä¸å­˜åœ¨ -> {file_path}")
            return sc.emptyRDD(), {}, 0, 0
        if not fs.isFile(path):
            print(f"âŒ é”™è¯¯ï¼šæŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶ -> {file_path}")
            return sc.emptyRDD(), {}, 0, 0
        
        # è·å–æ–‡ä»¶å¤§å°ï¼Œå¯è§†åŒ–å±•ç¤º
        file_size = fs.getFileStatus(path).getLen()
        file_size_mb = round(file_size / 1024 / 1024, 2)
        print(f"âœ… æ–‡ä»¶æ ¡éªŒé€šè¿‡ï¼š{file_path}ï¼ˆå¤§å°ï¼š{file_size_mb} MBï¼‰")
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å‰ç½®æ ¡éªŒå¤±è´¥ï¼š{e}")
        return sc.emptyRDD(), {}, 0, 0

    # ===================== ä¼˜åŒ–4ï¼šä¼˜åŒ–RDDè½¬æ¢é€»è¾‘ï¼Œå‡å°‘å†—ä½™æ“ä½œ =====================
    print_progress("å¼€å§‹è¯»å–å¹¶è½¬æ¢æ–‡ä»¶æ•°æ®")
    try:
        # ä¼˜åŒ–ï¼šå°†filterå’Œmapæ“ä½œåˆå¹¶ï¼Œå‡å°‘RDDä¾èµ–é“¾ï¼›ä½¿ç”¨flatMapé¿å…ç©ºæ•°æ®
        def parse_line(line):
            line = line.strip()
            if not line:
                return []
            parts = line.split(",")
            if len(parts) != 3:
                return []
            try:
                # ä¸€æ¬¡æ€§å®Œæˆæµ®ç‚¹è½¬æ¢ï¼Œé¿å…å¤šæ¬¡map
                return [(float(parts[0]), float(parts[1]), float(parts[2]))]
            except (ValueError, TypeError):
                return []
        
        # è¯»å–æ–‡ä»¶ï¼šä½¿ç”¨ä¼˜åŒ–åçš„åˆ†åŒºæ•°ï¼Œå¹¶è¡Œè¯»å–
        matrix_rdd = sc.textFile(file_path, partitions).flatMap(parse_line)
        
        # ä¼˜åŒ–ï¼šç”¨take(1)æ›¿ä»£é»˜è®¤take(1)ï¼Œè½»é‡éªŒè¯æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆä»…è¯»å–1æ¡æ•°æ®ï¼Œä¸è§¦å‘å…¨é‡åŠ è½½ï¼‰
        sample_data = matrix_rdd.take(1)
        if not sample_data:
            print(f"âŒ è­¦å‘Šï¼šæ–‡ä»¶ä¸­æ— æœ‰æ•ˆæ ¼å¼æ•°æ®ï¼ˆéœ€æ»¡è¶³row,col,valueæ ¼å¼ï¼‰")
            return sc.emptyRDD(), {}, 0, 0
        
        print(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶å¹¶è½¬æ¢æ•°æ®")
        print(f"ğŸ“Š æ–‡ä»¶åˆ†åŒºæ•°ï¼ˆä¼˜åŒ–åï¼‰: {matrix_rdd.getNumPartitions()}")
        print(f"ğŸ“Œ æ•°æ®æ ·æœ¬ï¼š{sample_data[0]}")
    except Exception as e:
        print(f"âŒ è¯»å–HDFSæ–‡ä»¶æˆ–æ•°æ®è½¬æ¢å¤±è´¥: {e}")
        return sc.emptyRDD(), {}, 0, 0

    # ===================== ä¼˜åŒ–5ï¼šå¹¶è¡Œè®¡ç®—çŸ©é˜µç»´åº¦ï¼Œé¿å…å•çº¿ç¨‹collecté˜»å¡ =====================
    print_progress("å¼€å§‹å¹¶è¡Œè®¡ç®—çŸ©é˜µç»´åº¦")
    try:
        # ä¼˜åŒ–ï¼šä½¿ç”¨zipWithIndexé¿å…å…¨é‡collectï¼Œå¹¶è¡Œè·å–æœ€å¤§/æœ€å°ç´¢å¼•
        # æ­¥éª¤1ï¼šå¹¶è¡Œæå–è¡Œ/åˆ—ç´¢å¼•ï¼ˆåˆ†å¸ƒå¼è®¡ç®—ï¼Œæå‡é€Ÿåº¦ï¼‰
        row_rdd = matrix_rdd.map(lambda x: int(round(x[0]))).cache()  # ç¼“å­˜é¿å…é‡å¤è®¡ç®—
        col_rdd = matrix_rdd.map(lambda x: int(round(x[1]))).cache()
        
        # æ­¥éª¤2ï¼šå¹¶è¡Œè®¡ç®—æœ€å¤§/æœ€å°ç´¢å¼•ï¼ˆæ— éœ€collectæ‰€æœ‰æ•°æ®ï¼Œå¤§å¹…æå‡å¤§æ–‡ä»¶é€Ÿåº¦ï¼‰
        max_row = row_rdd.max() if not row_rdd.isEmpty() else -1
        min_row = row_rdd.min() if not row_rdd.isEmpty() else -1
        max_col = col_rdd.max() if not col_rdd.isEmpty() else -1
        min_col = col_rdd.min() if not col_rdd.isEmpty() else -1
        
        # æ­¥éª¤3ï¼šè®¡ç®—çŸ©é˜µç»´åº¦ï¼Œæ¸…ç†ç¼“å­˜
        matrix_rows = max_row + 1 if max_row >= 0 else 0
        matrix_cols = max_col + 1 if max_col >= 0 else 0
        row_rdd.unpersist()  # é‡Šæ”¾ç¼“å­˜ï¼Œé¿å…å†…å­˜å ç”¨
        col_rdd.unpersist()
        
        # å¯è§†åŒ–ç»´åº¦ä¿¡æ¯
        if matrix_rows > 0 and matrix_cols > 0:
            print(f"âœ… è‡ªåŠ¨è¯†åˆ«çŸ©é˜µç»´åº¦: {matrix_rows} è¡Œï¼ˆæ•´æ•°ï¼‰ Ã— {matrix_cols} åˆ—ï¼ˆæ•´æ•°ï¼‰")
            print(f"ğŸ“Š åŸå§‹æµ®ç‚¹ç´¢å¼•èŒƒå›´ï¼š")
            print(f"   è¡Œç´¢å¼•ï¼š[{min_row}, {max_row}]")
            print(f"   åˆ—ç´¢å¼•ï¼š[{min_col}, {max_col}]")
        else:
            print(f"âš ï¸  è­¦å‘Šï¼šçŸ©é˜µ {file_path} æ— æœ‰æ•ˆç´¢å¼•ï¼Œè¿”å›0è¡Œ0åˆ—")
            print(f"   è¡Œç´¢å¼•èŒƒå›´ï¼š[{min_row}, {max_row}]")
            print(f"   åˆ—ç´¢å¼•èŒƒå›´ï¼š[{min_col}, {max_col}]")
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨è·å–çŸ©é˜µç»´åº¦å¤±è´¥: {e}")
        matrix_rows, matrix_cols = 0, 0

    # ===================== ä¼˜åŒ–6ï¼šä¼˜åŒ–BçŸ©é˜µå­—å…¸æ„å»ºï¼Œå‡å°‘å†—ä½™è½¬æ¢ =====================
    matrix_dict = {}
    if is_b_matrix and not matrix_rdd.isEmpty() and matrix_rows > 0 and matrix_cols > 0:
        print_progress("å¼€å§‹æ„å»ºBçŸ©é˜µBroadcastå­—å…¸")
        try:
            # ä¼˜åŒ–ï¼šç›´æ¥åœ¨mapä¸­å®Œæˆæ•´æ•°è½¬æ¢ï¼Œé¿å…åç»­é‡å¤è®¡ç®—
            def map_to_col_key(x):
                col_idx = int(round(x[1]))
                key = (x[0], x[1])
                return (col_idx, (key, x[2]))
            
            # æŒ‰åˆ—åˆ†ç»„ï¼Œé«˜æ•ˆæ„å»ºå­—å…¸
            matrix_dict = matrix_rdd.map(map_to_col_key).groupByKey().collectAsMap()
            # æ‰¹é‡è½¬æ¢æ ¼å¼ï¼Œæå‡æ•ˆç‡
            matrix_dict = {k: dict(v) for k, v in matrix_dict.items()}
            
            print(f"âœ… æˆåŠŸç”ŸæˆBçŸ©é˜µBroadcastå­—å…¸")
            print(f"ğŸ“Š å­—å…¸ä¿¡æ¯ï¼šåŒ…å« {len(matrix_dict)} åˆ—æ•°æ®ï¼Œè¦†ç›–çŸ©é˜µæ‰€æœ‰åˆ—")
        except Exception as e:
            print(f"âŒ ç”ŸæˆBçŸ©é˜µBroadcastå­—å…¸å¤±è´¥: {e}")
            matrix_dict = {}

    print_progress("æ–‡ä»¶è¯»å–ä¸å¤„ç†æµç¨‹å®Œæˆ", "=")
    return matrix_rdd, matrix_dict, matrix_rows, matrix_cols
# ===================== 4. åŸç”ŸçŸ©é˜µä¹˜æ³•ï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼ŒRDD Joinå®ç°ï¼‰ =====================
def native_matrix_multiply(A_rdd, B_rdd, m: int, k: int, n: int) -> float:
    """
    åŸç”ŸçŸ©é˜µä¹˜æ³•ï¼ˆåŸºäºRDD Joinå®ç°ï¼‰ï¼Œå…¨æµ®ç‚¹å‹è¿ç®—
    :param A_rdd: çŸ©é˜µAçš„RDDï¼Œæ ¼å¼ä¸º(row_idx, col_idx, value)ï¼ˆå‡ä¸ºæµ®ç‚¹å‹ï¼‰
    :param B_rdd: çŸ©é˜µBçš„RDDï¼Œæ ¼å¼ä¸º(row_idx, col_idx, value)ï¼ˆå‡ä¸ºæµ®ç‚¹å‹ï¼‰
    :param m: çŸ©é˜µAçš„è¡Œæ•°ï¼ˆæ•´æ•°ï¼‰
    :param k: çŸ©é˜µAçš„åˆ—æ•°ï¼ˆæ•´æ•°ï¼Œç­‰äºBçš„è¡Œæ•°ï¼‰
    :param n: çŸ©é˜µBçš„åˆ—æ•°ï¼ˆæ•´æ•°ï¼‰
    :return: æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å›0.0
    """
    if A_rdd.isEmpty() or B_rdd.isEmpty():
        print("âŒ åŸç”ŸçŸ©é˜µä¹˜æ³•å¤±è´¥ï¼šAçŸ©é˜µæˆ–BçŸ©é˜µä¸ºç©º")
        return 0.0
    
    if k == 0:
        print("âŒ åŸç”ŸçŸ©é˜µä¹˜æ³•å¤±è´¥ï¼šå…³è”ç»´åº¦kä¸º0ï¼ˆçŸ©é˜µAçš„åˆ—æ•°ä¸çŸ©é˜µBçš„è¡Œæ•°ä¸åŒ¹é…ï¼‰")
        return 0.0
    
    print_progress("å¼€å§‹æ‰§è¡ŒåŸç”ŸçŸ©é˜µä¹˜æ³•ï¼ˆRDD Joinå®ç°ï¼‰")
    start_time = time.time()
    try:
        # æ­¥éª¤1ï¼šAçŸ©é˜µæŒ‰åˆ—ç´¢å¼•åˆ†ç»„ (col_idx, (row_idx, float_value))
        A_by_col = A_rdd.map(lambda x: (x[1], (x[0], x[2])))
        # æ­¥éª¤2ï¼šBçŸ©é˜µæŒ‰è¡Œç´¢å¼•åˆ†ç»„ (row_idx, (col_idx, float_value))
        B_by_row = B_rdd.map(lambda x: (x[0], (x[1], x[2])))
        # æ­¥éª¤3ï¼šJoinåè®¡ç®—æµ®ç‚¹ä¹˜ç§¯å¹¶èšåˆï¼ˆAçš„åˆ— = Bçš„è¡Œï¼‰
        product = A_by_col.join(B_by_row) \
            .map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1])) \
            .reduceByKey(lambda a, b: a + b)  # æµ®ç‚¹å‹ç´¯åŠ 
        # è§¦å‘è®¡ç®—ï¼ˆSparkè‡ªåŠ¨åˆ†é…èµ„æºæ‰§è¡Œï¼‰
        product_count = product.count()
        elapsed_time = time.time() - start_time
        
        print(f"\nâœ… ã€åŸç”ŸçŸ©é˜µä¹˜æ³•å®Œæˆï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼‰ã€‘")
        print(f"ç»“æœçŸ©é˜µå…ƒç´ æ•°: {product_count}")
        print(f"æ‰§è¡Œè€—æ—¶: {elapsed_time:.2f}s")
        print(f"çŸ©é˜µç»´åº¦ï¼šA({m}x{k}) Ã— B({k}x{n}) = ç»“æœ({m}x{n})")
        
        # é¢„è§ˆç»“æœçŸ©é˜µ
        preview_matrix_rdd(product, "åŸç”Ÿä¹˜æ³•ç»“æœ")
        
        return elapsed_time
    except Exception as e:
        print(f"âŒ åŸç”ŸçŸ©é˜µä¹˜æ³•æ‰§è¡Œå¤±è´¥: {e}")
        return 0.0

# ===================== 5. Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•ï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼Œåˆ†åŒºå†…é«˜æ•ˆè®¡ç®—ï¼‰ =====================
def broadcast_optimized_matrix_multiply(A_rdd, B_col_dict: dict, m: int, k: int, n: int) -> float:
    """
    Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•ï¼Œå…¨æµ®ç‚¹å‹è¿ç®—ï¼Œå‡å°‘ç½‘ç»œä¼ è¾“
    :param A_rdd: çŸ©é˜µAçš„RDDï¼Œæ ¼å¼ä¸º(row_idx, col_idx, value)ï¼ˆå‡ä¸ºæµ®ç‚¹å‹ï¼‰
    :param B_col_dict: çŸ©é˜µBçš„åˆ—å­—å…¸ï¼Œæ ¼å¼ä¸º{col_idx: {(row_idx, col_idx): float_value}}
    :param m: çŸ©é˜µAçš„è¡Œæ•°ï¼ˆæ•´æ•°ï¼‰
    :param k: çŸ©é˜µAçš„åˆ—æ•°ï¼ˆæ•´æ•°ï¼Œç­‰äºBçš„è¡Œæ•°ï¼‰
    :param n: çŸ©é˜µBçš„åˆ—æ•°ï¼ˆæ•´æ•°ï¼‰
    :return: æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å›0.0
    """
    if not B_col_dict or A_rdd.isEmpty():
        print("âŒ Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•å¤±è´¥ï¼šBçŸ©é˜µå­—å…¸ä¸ºç©ºæˆ–AçŸ©é˜µä¸ºç©º")
        return 0.0
    
    if k == 0:
        print("âŒ Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•å¤±è´¥ï¼šå…³è”ç»´åº¦kä¸º0ï¼ˆçŸ©é˜µAçš„åˆ—æ•°ä¸çŸ©é˜µBçš„è¡Œæ•°ä¸åŒ¹é…ï¼‰")
        return 0.0
    
    print_progress("å¼€å§‹æ‰§è¡ŒBroadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•ï¼ˆåˆ†åŒºå†…é«˜æ•ˆè®¡ç®—ï¼‰")
    start_time = time.time()
    try:
        # å¹¿æ’­BçŸ©é˜µå­—å…¸ï¼ˆä»…ä¼ è¾“ä¸€æ¬¡åˆ°æ‰€æœ‰Executorï¼‰
        b_broadcast = A_rdd.context.broadcast(B_col_dict)
        print(f"âœ… BçŸ©é˜µå­—å…¸å·²å¹¿æ’­åˆ°Executorï¼Œå­—å…¸å¤§å°ï¼š{len(B_col_dict)} åˆ—")
        
        def compute_partition(iter):
            """åˆ†åŒºå†…æµ®ç‚¹å‹è®¡ç®—ï¼šæŒ‰åˆ—æŸ¥æ‰¾BçŸ©é˜µï¼Œå‡å°‘æ— æ•ˆéå†"""
            b_dict = b_broadcast.value
            result = {}
            # éå†AçŸ©é˜µçš„æ¯ä¸ªæµ®ç‚¹å‹å…ƒç´  (row_idx, col_idx, float_value)
            for (i, j, a_val) in iter:
                # åˆ—ç´¢å¼•è½¬æ¢ä¸ºæ•´æ•°ï¼ŒåŒ¹é…BçŸ©é˜µå­—å…¸çš„key
                j_int = int(round(j))
                if j_int not in b_dict:
                    continue
                # éå†BçŸ©é˜µä¸­å¯¹åº”åˆ—çš„æ‰€æœ‰æµ®ç‚¹å‹å…ƒç´ 
                for (bk_row, bk_col), b_val in b_dict[j_int].items():
                    # æµ®ç‚¹å‹ç´¯åŠ ä¹˜ç§¯ï¼šC[i][bk_col] += A[i][j] * B[bk_row][bk_col]
                    result_key = (i, bk_col)
                    result[result_key] = result.get(result_key, 0.0) + a_val * b_val
            return result.items()
        
        # åˆ†åŒºè®¡ç®—+èšåˆï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼ŒSparkè‡ªåŠ¨åˆ†é…æœ€å¤§å¹¶è¡Œåº¦ï¼‰
        product = A_rdd.mapPartitions(compute_partition).reduceByKey(lambda a, b: a + b)
        product_count = product.count()
        # é‡Šæ”¾Broadcastèµ„æºï¼Œé¿å…å†…å­˜æ³„æ¼
        b_broadcast.unpersist(blocking=True)
        print(f"âœ… Broadcastèµ„æºå·²é‡Šæ”¾")
        
        elapsed_time = time.time() - start_time
        print(f"\nâœ… ã€Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•å®Œæˆï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼‰ã€‘")
        print(f"ç»“æœçŸ©é˜µå…ƒç´ æ•°: {product_count}")
        print(f"æ‰§è¡Œè€—æ—¶: {elapsed_time:.2f}s")
        print(f"çŸ©é˜µç»´åº¦ï¼šA({m}x{k}) Ã— B({k}x{n}) = ç»“æœ({m}x{n})")
        
        # é¢„è§ˆç»“æœçŸ©é˜µ
        preview_matrix_rdd(product, "Broadcastä¼˜åŒ–ä¹˜æ³•ç»“æœ")
        
        return elapsed_time
    except Exception as e:
        print(f"âŒ Broadcastä¼˜åŒ–çŸ©é˜µä¹˜æ³•æ‰§è¡Œå¤±è´¥: {e}")
        return 0.0

# ===================== 6. æµ‹è¯•ä¸»å‡½æ•°ï¼ˆæ”¯æŒé€‰æ‹©ä¹˜æ³•æ–¹å¼ï¼Œå…¨æµ®ç‚¹å‹é€‚é…ï¼‰ =====================
def run_tests(choose_multiply: str = "both"):
    """
    æµ‹è¯•ä¸»å‡½æ•°ï¼Œæ”¯æŒé€‰æ‹©ä¹˜æ³•æ–¹å¼ï¼Œå…¨æµ®ç‚¹å‹çŸ©é˜µè¿ç®—
    :param choose_multiply: å¯é€‰å€¼ï¼š"native"ï¼ˆä»…åŸç”Ÿä¹˜æ³•ï¼‰ã€"broadcast"ï¼ˆä»…ä¼˜åŒ–ä¹˜æ³•ï¼‰ã€"both"ï¼ˆä¸¤è€…éƒ½æ‰§è¡Œï¼‰
    """
    # åˆå§‹åŒ–Sparké›†ç¾¤
    spark = init_spark()
    sc = spark.sparkContext
    
    # é…ç½®HDFSçŸ©é˜µæ–‡ä»¶è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ çš„å®é™…HDFSè·¯å¾„ï¼Œå·²ä¿®æ­£å ä½ç¬¦ï¼‰
    HDFS_MATRIX_DIR = "hdfs://master:9000/user/yourname/matrix_data"  # yournameæ›¿æ¢ä¸ºå®é™…ç”¨æˆ·åsparkuser
    A_file = os.path.join(HDFS_MATRIX_DIR, "matrix_2000_5_A.txt")
    B_file = os.path.join(HDFS_MATRIX_DIR, "matrix_2000_5_B.txt")
    
    # æ‰“å°æµ‹è¯•é…ç½®
    print_progress("å¼€å§‹çŸ©é˜µä¹˜æ³•æµ‹è¯•ï¼ˆå…¨æµ®ç‚¹å‹é€‚é…ï¼‰")
    print(f"AçŸ©é˜µHDFSè·¯å¾„: {A_file}")
    print(f"BçŸ©é˜µHDFSè·¯å¾„: {B_file}")
    print(f"é€‰æ‹©æ‰§è¡Œçš„ä¹˜æ³•æ–¹å¼: {choose_multiply.upper()}")
    
    # è¯»å–çŸ©é˜µæ–‡ä»¶ï¼ˆå…¨æµ®ç‚¹å‹è§£æï¼Œè‡ªåŠ¨è®¡ç®—æ•´æ•°ç»´åº¦ï¼‰
    print_progress("å¼€å§‹è¯»å–çŸ©é˜µA", "-")
    A_rdd, _, m, k = read_matrix_from_file_txt(
        sc, 
        A_file, 

    )  

    # ç¤ºä¾‹2ï¼šB.csvæ˜¯å•è¡Œ50åˆ—çŸ©é˜µï¼ˆéœ€ä½œä¸ºBçŸ©é˜µï¼Œå¼€å¯is_b_matrix=Trueï¼‰
    print_progress("å¼€å§‹è¯»å–çŸ©é˜µB", "-")
    B_rdd, B_col_dict, k_b, n = read_matrix_from_file_txt(
        sc, 
        B_file, 
        is_b_matrix=True, 

    )
    
    # æ‰“å°çŸ©é˜µåŸºæœ¬ä¿¡æ¯
    print_progress("çŸ©é˜µè¯»å–å®Œæˆï¼ŒéªŒè¯åˆæ³•æ€§")
    print_matrix_info("çŸ©é˜µA", m, k, A_rdd.count() if not A_rdd.isEmpty() else 0)
    print_matrix_info("çŸ©é˜µB", k_b, n, B_rdd.count() if not B_rdd.isEmpty() else 0)
    
    # éªŒè¯çŸ©é˜µä¹˜æ³•åˆæ³•æ€§ï¼šAçš„åˆ—æ•°å¿…é¡»ç­‰äºBçš„è¡Œæ•°ï¼ˆæ•´æ•°å¯¹æ¯”ï¼‰
    if k != k_b:
        print(f"âŒ çŸ©é˜µä¹˜æ³•ä¸åˆæ³•ï¼šAçŸ©é˜µçš„åˆ—æ•°({k}) â‰  BçŸ©é˜µçš„è¡Œæ•°({k_b})ï¼Œæ— æ³•è¿›è¡Œä¹˜æ³•è¿ç®—")
        spark.stop()
        return
    
    if A_rdd.isEmpty() or B_rdd.isEmpty():
        print(f"âŒ çŸ©é˜µæ–‡ä»¶è¯»å–å¤±è´¥æˆ–ä¸ºç©ºï¼Œç»ˆæ­¢æµ‹è¯•")
        spark.stop()
        return
    
    print(f"âœ… çŸ©é˜µåˆæ³•æ€§éªŒè¯é€šè¿‡")
    print(f"çŸ©é˜µè¿ç®—ç»´åº¦ï¼šA({m}x{k}) Ã— B({k}x{n}) = ç»“æœ({m}x{n})")
    print_progress("å¼€å§‹æ‰§è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—")
    
    native_time = 0.0
    broadcast_time = 0.0
    
    # æ ¹æ®é€‰æ‹©æ‰§è¡Œå¯¹åº”çš„ä¹˜æ³•å‡½æ•°
    if choose_multiply in ["native", "both"]:
        native_time = native_matrix_multiply(A_rdd, B_rdd, m, k, n)
    
    if choose_multiply in ["broadcast", "both"]:
        broadcast_time = broadcast_optimized_matrix_multiply(A_rdd, B_col_dict, m, k, n)
    
    # æ€§èƒ½å¯¹æ¯”ï¼ˆä»…å½“ä¸¤è€…éƒ½æ‰§è¡Œä¸”è€—æ—¶æœ‰æ•ˆæ—¶ï¼‰
    print_progress("æµ‹è¯•å®Œæˆï¼Œæ±‡æ€»ç»“æœ")
    if choose_multiply == "both" and broadcast_time > 0 and native_time > 0:
        speedup = (native_time - broadcast_time) / native_time * 100
        print(f"\nğŸ“Š ã€æ€§èƒ½å¯¹æ¯”ç»“æœï¼ˆå…¨æµ®ç‚¹å‹è¿ç®—ï¼‰ã€‘")
        print(f"åŸç”ŸçŸ©é˜µä¹˜æ³•è€—æ—¶: {native_time:.2f}s")
        print(f"Broadcastä¼˜åŒ–ä¹˜æ³•è€—æ—¶: {broadcast_time:.2f}s")
        print(f"æ€§èƒ½æå‡æ¯”ä¾‹: {speedup:.2f}%")
        print(f"ç»“è®ºï¼šBroadcastä¼˜åŒ–æ¯”åŸç”Ÿå®ç°å¿« {native_time - broadcast_time:.2f} ç§’")
    else:
        print(f"\nğŸ“Š ã€æµ‹è¯•å®Œæˆã€‘")
        print(f"æ‰§è¡Œæ–¹å¼ï¼š{choose_multiply.upper()}")
        if native_time > 0:
            print(f"åŸç”ŸçŸ©é˜µä¹˜æ³•è€—æ—¶: {native_time:.2f}s")
        if broadcast_time > 0:
            print(f"Broadcastä¼˜åŒ–ä¹˜æ³•è€—æ—¶: {broadcast_time:.2f}s")
        print(f"æ— éœ€è¿›è¡Œæ€§èƒ½å¯¹æ¯”ï¼ˆä»…æ‰§è¡Œäº†å•ä¸€ä¹˜æ³•æ–¹å¼æˆ–è€—æ—¶æ— æ•ˆï¼‰")
    
    # åœæ­¢Sparkä¼šè¯ï¼Œé‡Šæ”¾èµ„æº
    print_progress("åœæ­¢Sparké›†ç¾¤ï¼Œé‡Šæ”¾èµ„æº")
    spark.stop()
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•æµç¨‹ç»“æŸï¼ŒSparké›†ç¾¤å·²æ­£å¸¸åœæ­¢")

# ===================== å…¥å£å‡½æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æˆ–ç›´æ¥æŒ‡å®šï¼‰ =====================
if __name__ == "__main__":
    # Windows Dockerç‰¹æ®Šå¤„ç†
    if os.name == "nt":
        os.environ["SPARK_DRIVER_HOST"] = "host.docker.internal"
    
    # æ–¹å¼1ï¼šç›´æ¥åœ¨ä»£ç ä¸­æŒ‡å®šè°ƒç”¨æ–¹å¼ï¼ˆå¯é€‰ï¼š"native"ã€"broadcast"ã€"both"ï¼‰
    #run_tests(choose_multiply="native")  # ä»…æ‰§è¡ŒåŸç”ŸçŸ©é˜µä¹˜æ³•
    run_tests(choose_multiply="broadcast")  # ä»…æ‰§è¡ŒBroadcastä¼˜åŒ–ä¹˜æ³•
    #run_tests(choose_multiply="both")  # ä¸¤è€…éƒ½æ‰§è¡Œï¼ˆé»˜è®¤ï¼‰åªæ”¹è·å–è¡Œåˆ—çš„éƒ¨åˆ†ï¼Œåˆ«çš„ä¸ç”¨æ”¹ æ·»åŠ å¯è§†åŒ–è¿è¡Œ